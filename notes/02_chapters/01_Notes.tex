\epigraph{"..."}
{\textit{.}}

\section{Convolutional- \& Recurrent Neural Networks}
\begin{multicols}{2}
\subsection{Convolutional Nets}
Issue with vanilla feed-forward neural network
\begin{itemize}
  \item Data squeezed into a 1xN vector.
  \item Single operation on whole input.
  \item No attention to \textbf{spatial adjacency}.
\end{itemize}
Solution: 'Slide' weight matrix over a part of the input. E.g.
\begin{itemize}
  \item Input image: 32x32x3 (i.e. RGB-colors)
  \item Filter: 5x5x3 $\rightarrow$ dot product $\rightarrow$ 1 number
  \item Activation map: 28x28x1 (convolved over all spatial locations)
  \begin{itemize}
    \item One activation map for each filter. These are stacked.
    \item E.g. 28x28x6 for 6 5x5x3 filters.
    \begin{itemize}
      \item An additional activation map can be added by using a filter on the activation map of the former layer.
      \item E.g. a new activation map layer: 24x24x10 by using 10 5x5x3 filters.
    \end{itemize}
  \end{itemize}
\end{itemize}
\textbf{Stride}
\begin{itemize}
  \item How many pixels are moved each time, e.g. 1.
\end{itemize}
\textbf{Padding}
\begin{itemize}
  \item The no. zeroes added at the borders of the input image before applying the filter.
  \item Full padding: If you don't want the image to shrink.
  \item Half padding: The image shrinks less, e.g. padding = 1.
\end{itemize}
Width of the activation map will be
\begin{align*}
  W=\frac{N+2P-F}{S}+1
\end{align*}
E.g. for the example above with padding=2
\begin{align*}
 W=\frac{32+2\cdot2-5}{1}+1=32
\end{align*}
\textbf{Pooling:} Condense the information by downsampling.
\begin{itemize}
  \item Speeds up the learning process.
  \item 'max pool' where only largest value is saved for each quadrant as they are expected to contain the more important signal.
  \item 'average pooling'
  \item E.g. 16x16 $\rightarrow$ 4x4
  \item Doesn't shrink it in the depth direction.
\end{itemize}

\end{multicols}

\subsection{Recurrent Nets}
\begin{multicols}{2}
Optimal for \textbf{sequential data} but useful for non-sequential data as well.
\\ \\
Issue for \textbf{all} feed forward neural networks
\begin{itemize}
  \item Input and output must be of hard-assigned dimensions.
\end{itemize}
Backpropagation: Compute the sum of losses.
\begin{itemize}
  \item Can get complicated as the number becomes very high up with long chains.
  \item Can break it into parts.
\end{itemize}

\begin{itemize}
  \item f: Forget vector (0 = forget, 1 = remember) $\rightarrow$ what to forget from c-vector
  \begin{itemize}
    \item c: New hidden vector, i.e. long term memory
    \item c is not converted, $\rightarrow$ not multiplied by weight $\rightarrow$ gradient doesn't explode $\rightarrow$ much quicker
  \end{itemize}
  \item i, g: adds to memory vector, c
  \item o: output vector
\end{itemize}



\end{multicols}

%\includegraphics[width = 1.0\textwidth]{CO2.PNG}
