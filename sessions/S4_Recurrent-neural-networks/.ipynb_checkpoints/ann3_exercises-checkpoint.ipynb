{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE tsds folder**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Artificial Neural Networks 3\n",
    "\n",
    "*Wednesday, February 26, 2019*\n",
    "\n",
    "We learned about feedforward neural networks over the last two weeks, along with most of the fundamental maths, algorithms, tricks, etc. that go into training them. Knowing how signal propagates forward, how gradients flow backwards and how weights get updates, we are now ready to release our imagination and create some deep neural networks that can do remarkable things. We will focus on two of the most **powerful architectures**, the Convolutional Neural Network (CNN) and the Recurrent Neural Network (RNN), which have revolutionalized image recognition and sequence modeling across all fields over the last 10 years.\n",
    "\n",
    "- Part 4.1: Convolutional Neural Networks\n",
    "- Part 4.2: Recurrent Neural Networks\n",
    "\n",
    "**Questions**: Outside of class, use [issue](https://github.com/abjer/tsds/issues) on GitHub for asking questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T14:02:03.114432Z",
     "start_time": "2019-02-26T14:02:01.813112Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mnist_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4883bd17e08c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmnist_loader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mnist_loader'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import mnist_loader\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.1: Convolutional Neural Networks\n",
    "\n",
    "The problem with feed forward images when classifying images is that the input layer looks at the whole image at the same time. Each input neuron thus becomes associated with a specific pixel, and as the network learns it may grow to expect a certain signal to emerge at that specific pixel. But if you imagine an image of a cat, features, like the whiskers, which make it cat-like are not bound to a specific *place* in the image â€“ they can be in the centre, top corner, or any other place.\n",
    "\n",
    "CNNs solve this problem, with what's called *convolutional layers*. A convolutional input layer, for example, doesn't have a single weight for each input value. Instead, it has one or more much smaller *filters*, each one a set of input neurons (often $3 \\times 3 \\times d$ where $d$ is the depth of the input image) that gets *convolved* across the input image to produce a new image, called an *activation map*. [Here](https://github.com/vdumoulin/conv_arithmetic) are some nice gifs that illustrate different ways one can convolve a weight matrix across an input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pen and paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get your intuition for computations on input data in CNNs fine-tuned, I have a few small quizzes for you. First, we'll consider the size of the parameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.1**: Imagine you have a CNN with just one convolutional layer with a single filter. All it does, is take an input image and produce an activation map. The dimensionality of the filter in your convolutional layer is $5 \\times 5 \\times 3$. How many weights (or *parameters*) are there in this model?\n",
    ">\n",
    "> *Hint*: Don't forget the bias!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the formula for computing the size of the activation map resulting from a convolution. \n",
    "If you have filter that is $F$ wide, your input image is $W_0$ wide, you are padding the sizes by\n",
    "$P$ and your stride is $S$, the resulting image will have width:\n",
    "\n",
    "$$ W_1 = \\frac{W_0 - F + 2P}{S} + 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.2**: You input an image of dimensions $28 \\times 28 \\times 3$, use a padding of 2, a stride of 1,\n",
    "and then slide your $5 \\times 5 \\times 3$ filter across the image. What is the dimensionality of the resulting activation map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.3**: Let's say you now want to use a stride of 2, instead of 1. What problem does immediately cause?\n",
    "How can we solve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Maxpooling* is a method used a lot in CNNs, which downsamples the size of an activation map. It's used primarily to reduce amount of parameters and computations in the network, and to avoid overfitting. Here's an illustration of how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://cs231n.github.io/assets/cnn/maxpool.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, for each $2 \\times 2$ square in your activation map, you pick the largest value in that square. You do this independently for every depth slice in your activation map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In PyTorch, the dimension of data is a little different from what you may expect. The first index,\n",
    "indexes datapoints, the second *channels*, and the last two are the dimensions of the input data. So if\n",
    "you have a batch of data containing 100 datapoints, each one an RGB image (so 3 channels: red, green, blue)\n",
    "with resolution $128 \\times 128$, then the dimensionality of your input data is (100, 3, 128, 128)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.4**: Given the activation map below, what is the corresponding activation map after maxpooling ($2 \\times 2$ filter, stride 2)? Report the shape of the resulting activation map.\n",
    ">\n",
    "> *Hint*: Don't try to do this manually, it will be a nightmare. Instead, look for functionality in PyTorch's API that will help you along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:57:28.380151Z",
     "start_time": "2019-02-22T14:57:28.375639Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "activation_map = torch.randn(10, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T14:58:59.851570Z",
     "start_time": "2019-02-22T14:58:59.846594Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 14, 14])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution\n",
    "m = torch.nn.MaxPool2d((2, 2), stride=2)\n",
    "m(activation_map).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNNs in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways you can define your neural network models using PyTorch. Last week I gave an example of how\n",
    "to do it using `Sequential`, which is probably the easiest way to do it. But you can also write the network as\n",
    "a Python class which gives you a bit more freedom. Below I've given an example that implements the same network\n",
    "in two different ways. The code below assumes input images of shape $28 \\times 28 \\times 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T12:39:04.777948Z",
     "start_time": "2019-02-25T12:39:04.767474Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1:\n",
    "# ----------\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    Flatten(),\n",
    "    nn.Linear(10 * 14 * 14, 10)\n",
    ")\n",
    "\n",
    "# Example 2:\n",
    "# ----------\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 10, 3, 1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(10 * 14 * 14, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 10 * 14 * 14)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way is easier to read, but the second way offers a bit more design flexibility, which we will need later. For example, when using\n",
    "`Sequential`, in order to flatten the array of neurons resulting from the convolution, so we could apply a linear\n",
    "layer, we had to define a `Flatten` class. With the second way we can just reshape it with `view`.\n",
    "\n",
    "In the following exercises you choose yourself how to write your networks, whether with `Sequential` or as a class.\n",
    "\n",
    "We will use the MNIST dataset one last time now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T10:41:00.132899Z",
     "start_time": "2019-02-25T10:40:59.071015Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data()\n",
    "\n",
    "# Training data\n",
    "x = torch.from_numpy(training_data[0])\n",
    "y_ = torch.from_numpy(training_data[1])\n",
    "y = torch.zeros(x.shape[0], 10)\n",
    "y[torch.arange(x.shape[0]), y_] = 1\n",
    "\n",
    "# Test data\n",
    "x_test = torch.from_numpy(test_data[0])\n",
    "y_test_ = torch.from_numpy(test_data[1])\n",
    "y_test = torch.zeros(x_test.shape[0], 10)\n",
    "y_test[torch.arange(x_test.shape[0]), y_test_] = 1\n",
    "\n",
    "# Reshape to comply with PyTorch standard input format\n",
    "x = x.view(-1, 1, 28, 28)            # (n_datapoints, n_channels, height, width)\n",
    "x_test = x_test.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.5**: Implement Nielsen's [last convolutional neural network](http://neuralnetworksanddeeplearning.com/chap6.html#convolutional_neural_networks_in_practice)\n",
    "(the one with two convolutional layers and dropout), and score an accuracy higher than 98%. It doesn't have to be\n",
    "fully identical, but his solution is pretty great, so getting close is a cheap way to score a high accuracy.\n",
    ">\n",
    "> *Hint:* The cost function `torch.nn.MSELoss(reduction='mean')` works great with `learning_rate=1e-3` or less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is a great benchmark because it allows us to directly measure the performance of a neural\n",
    "network against the human eye. But often, the association between datapoint and label is not as clear, yet we\n",
    "still want to do prediction.\n",
    "\n",
    "I went on [Kaggle.com](www.kaggle.com) and looked for some *harder* datasets. At the time of writing there's\n",
    "[a competetion](https://www.kaggle.com/c/petfinder-adoption-prediction) where you can win up to $\\$$10.000 by\n",
    "predicting how quickly pets get adopted (on a pet-adoption site) based on meta data and images of the pets. From that competetion, I prepared a dataset of $128 \\times 128$ images along with adoption time (lower is better, see description on Kaggle). In the actual competetion people are using more data sources than just image, but for\n",
    "now, let's see how well we can do with just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.6**: Load the prepared dataset and build a CNN that predicts adoption-time category from the image.\n",
    "Leave out the last 1000 datapoints from training and save them for validation. You have complete freedom in how\n",
    "you want to construct the neural network (choise of cost function, inner architecture, training hyperparameters,\n",
    "etc.). Report an accuracy on the test set (last 1000 points) that's higher than the baseline (0.273). Using your\n",
    "adoptability predictor, visualize the 5 most adoptable dogs.\n",
    ">\n",
    "> As an extra bonus, I also included a test dataset with no corresponding target array. You can take your model and try to predict the test data, and format it similar to the `sample_submission.csv` file and submit your predictions to the competetion to get an estimate of the true performance of your model. Let me know what you score if you do that. And if you choose to join the competetion and try to win the money, I'd also like to hear about your progress.\n",
    ">\n",
    "> Also, note that to get high accuracy, you need to make a network that is bigger than what you had in the previous exercise. This will take a long time to train (hours at least). If you have a computer with a small\n",
    "CPU you will have to make do with a smaller neural network, and thus a lower accuracy. But you should still be able to score above the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T12:35:26.844065Z",
     "start_time": "2019-02-25T12:35:24.312014Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "x = torch.from_numpy(np.load('x.npy').astype(np.float32))\n",
    "y_ = torch.from_numpy(np.load('y.npy').astype(int))\n",
    "y = torch.zeros(x.shape[0], 5)\n",
    "y[torch.arange(x.shape[0]), y_] = 1\n",
    "\n",
    "# Split into train and test\n",
    "x_test = x[-1000:]\n",
    "y_test = y[-1000:]\n",
    "x = x[:-1000]\n",
    "y = y[:-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T12:36:44.547242Z",
     "start_time": "2019-02-25T12:36:44.543041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2729)\n"
     ]
    }
   ],
   "source": [
    "# Baseline\n",
    "print(max(y.sum(0))/y.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.2: Recurrent Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you may have noticed the obvious limitation to vanilla neural networks and CNNs, which is that they\n",
    "are extremely contrained in their processing of data. Input and output must be of a specific size and computation\n",
    "happens over a fixed number of layers. Recurrent Neural Networks (RNNs) offer an exciting alternative to this\n",
    "constrained computing paradigm, as they model data in **sequence**. There is no inherent constraint to how long\n",
    "or short sequences can be, and likewise for outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training RNNs looks slightly different than feed forward networks, because we have to reuse outputs of each\n",
    "iteration as inputs to the next iteration. Therefore I have given an example below where I fit an RNN to\n",
    "Shakespeare's Hamlet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T15:46:34.095402Z",
     "start_time": "2019-02-25T15:46:32.903496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the data (thanks MIT)\n",
    "response = rq.get(\"http://shakespeare.mit.edu/hamlet/full.html\")\n",
    "hamlet = BeautifulSoup(response.content, \"html.parser\").getText()\n",
    "\n",
    "# Convert text to character-level one-hot encoding\n",
    "hamlet_one_hot = pd.get_dummies(pd.Series(list(hamlet)))\n",
    "character_vec = hamlet_one_hot.columns\n",
    "x = torch.from_numpy(hamlet_one_hot.values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-26T13:59:18.294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd4e8fc75e54fa1a6fd43735fbff665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | loss: 837.5260009765625 | fraction correct: 0.02\n",
      "10 | loss: 758.2477416992188 | fraction correct: 0.12\n",
      "20 | loss: 688.7192993164062 | fraction correct: 0.15\n",
      "30 | loss: 577.0355834960938 | fraction correct: 0.245\n",
      "40 | loss: 546.7378540039062 | fraction correct: 0.21\n",
      "50 | loss: 501.7413635253906 | fraction correct: 0.33\n",
      "60 | loss: 483.5766296386719 | fraction correct: 0.295\n",
      "70 | loss: 494.6862487792969 | fraction correct: 0.325\n",
      "80 | loss: 497.8192138671875 | fraction correct: 0.28\n",
      "90 | loss: 472.5584716796875 | fraction correct: 0.355\n",
      "100 | loss: 495.93133544921875 | fraction correct: 0.31\n",
      "110 | loss: 442.7196044921875 | fraction correct: 0.345\n",
      "120 | loss: 405.5710754394531 | fraction correct: 0.49\n",
      "130 | loss: 469.5902404785156 | fraction correct: 0.34\n",
      "140 | loss: 452.2145080566406 | fraction correct: 0.325\n",
      "150 | loss: 401.5863342285156 | fraction correct: 0.485\n",
      "160 | loss: 442.0084228515625 | fraction correct: 0.38\n",
      "170 | loss: 487.2173156738281 | fraction correct: 0.3\n",
      "180 | loss: 465.43310546875 | fraction correct: 0.3\n",
      "190 | loss: 447.5295715332031 | fraction correct: 0.32\n",
      "200 | loss: 429.32696533203125 | fraction correct: 0.4\n",
      "210 | loss: 420.3609924316406 | fraction correct: 0.39\n",
      "220 | loss: 322.02435302734375 | fraction correct: 0.535\n",
      "230 | loss: 439.0169982910156 | fraction correct: 0.35\n",
      "240 | loss: 480.1730041503906 | fraction correct: 0.34\n",
      "250 | loss: 320.5889892578125 | fraction correct: 0.57\n",
      "260 | loss: 378.3502502441406 | fraction correct: 0.49\n",
      "270 | loss: 425.478759765625 | fraction correct: 0.41\n",
      "280 | loss: 450.3674621582031 | fraction correct: 0.375\n",
      "290 | loss: 387.3787841796875 | fraction correct: 0.43\n",
      "300 | loss: 417.56182861328125 | fraction correct: 0.38\n",
      "310 | loss: 379.59759521484375 | fraction correct: 0.46\n",
      "320 | loss: 391.06146240234375 | fraction correct: 0.425\n",
      "330 | loss: 389.7279357910156 | fraction correct: 0.38\n",
      "340 | loss: 401.16387939453125 | fraction correct: 0.445\n",
      "350 | loss: 341.48223876953125 | fraction correct: 0.535\n",
      "360 | loss: 381.73187255859375 | fraction correct: 0.445\n",
      "370 | loss: 428.1701965332031 | fraction correct: 0.4\n",
      "380 | loss: 412.9840087890625 | fraction correct: 0.365\n",
      "390 | loss: 353.45989990234375 | fraction correct: 0.485\n",
      "400 | loss: 341.12127685546875 | fraction correct: 0.525\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Neural network layers. Rather than giving the input as a one-hot\n",
    "        # vector, we represent it as a point in a high-dimensional space (i.e.\n",
    "        # \"an embedding\"). This tends to work better.\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_, hidden):\n",
    "        signal = self.encoder(input_).view(1, 1, -1)  # Embed input character\n",
    "        output, hidden = self.rnn(signal, hidden)     # Get output and hidden vector(s)\n",
    "        prediction = self.decoder(output.view(1, -1))     # Decode to \"prediction\" vector\n",
    "        \n",
    "        return prediction, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (\n",
    "            torch.autograd.Variable(torch.zeros(self.n_layers, 1, self.hidden_size)),\n",
    "            torch.autograd.Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        )\n",
    "    \n",
    "epochs = 5000\n",
    "seq_len = 200\n",
    "learning_rate = 1e-2\n",
    "n_layers = 2\n",
    "\n",
    "model = RNN(len(character_vec), 100, len(character_vec), n_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# We're collecting the losses so we can plot how they (hopefully) decrease over time\n",
    "all_losses = []\n",
    "fraction_correct = []\n",
    "for t in tqdm(range(epochs)):\n",
    "    \n",
    "    # Initiate a hidden vector for \n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    # Pick a random input and output sequence. Here we are only taking one sequence\n",
    "    # per epoch, but normally people take a batch of sequences. Check https://github\n",
    "    # .com/spro/char-rnn.pytorch for an example on how to do that\n",
    "    i = np.random.randint(0, x.size(0)-seq_len)\n",
    "    input_ = torch.max(x[i:i+seq_len], 1)[1]      # torch.max(...)[1] gets the character in terms of its index, so\n",
    "    target = torch.max(x[i+1:i+1+seq_len], 1)[1]  # input_ will be something like torch.tensor([2, 1, 4, 0, ... 1])\n",
    "    \n",
    "    # Backprop through time. Here we are just summing the losses from each timestep\n",
    "    # and do backpropagation on the variable that holds the sum. PyTorch allows for that\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for j in range(input_.size(0)):\n",
    "        output, hidden = model(input_[j], hidden)\n",
    "        loss += loss_fn(output.view(1, -1), target[j].view(1, ))\n",
    "        correct += int(torch.max(output.view(1, -1), 1)[1][0] == target[j])\n",
    "    \n",
    "    # SGD step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Collect loss value for plot\n",
    "    all_losses.append(float(loss))\n",
    "    fraction_correct.append(correct / input_.size(0))\n",
    "        \n",
    "    # Progress\n",
    "    if t % 10 == 0:\n",
    "        print(t, \"| loss:\", float(loss), \"| fraction correct:\", fraction_correct[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:47:03.991643Z",
     "start_time": "2019-02-26T13:47:03.807094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb28335390>]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks the network learns! But we could have probably run it for a bit longer and gotten a better result. I'll\n",
    "leave that to you. But let's see what it can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.2.1**: Train the network for a while (the longer the better) until you feel its error has settled in some local minimum. Then go ahead and generate some gibberish Hamlet with it! To get better results, you can \"warm up\" the hidden state vectors by first running a sequence of actual Shapespeare through it and then starting generating from the last word in that sequence. Also, what I mean by \"start generating\" is that instead of predicting output from inputs drawn from your dataset, you input the prediction from the previous timestep and repeat, thus getting something that's completely made up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
