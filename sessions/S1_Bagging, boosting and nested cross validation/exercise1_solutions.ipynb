{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Set 1: Lending club\n",
    "\n",
    "*February 5, 2019*\n",
    "\n",
    "In this Exercise Set 1 we will investigate loan data from `Lending Club`. We will see how nested cross validation can be used  to compute a distribution of performance metrics for comparing different models. We will also try out bagging.\n",
    "\n",
    "Below we provide some code that helps by downloading and cleaning the data. Note this takes around 1 min if you have fast internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: there are three .zip files with letter = a,b,c. \n",
    "# To ensure the files download in reasonable time we \n",
    "# only work with the first of the three. If you have time\n",
    "# you can modify this cell to download all three. \n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "filenames = []\n",
    "base_url = 'https://resources.lendingclub.com/'\n",
    "\n",
    "letter = 'a'\n",
    "filename = f'LoanStats3{letter}.csv.zip'\n",
    "url = base_url+filename\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(filename, 'wb').write(r.content)\n",
    "filenames.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target we are going to predict is whether loan is repaid. This can be extracted from the `loan_status` below. Features names that we work with:\n",
    "\n",
    "- **annual_inc**: The self-reported annual income provided by the borrower during registration.\n",
    "- **dti**: A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n",
    "- **emp_length**: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years. \n",
    "- **grade**: LC assigned loan grade\n",
    "- **home_ownership**: The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n",
    "- **int_rate**: Interest Rate on the loan\n",
    "- **term**: The number of payments on the loan. Values are in months and can be either 36 or 60.\n",
    "- **verification_status**: Indicates if income was verified by LC, not verified, or if the income source was verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>...</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>hardship_flag</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>debt_settlement_flag_date</th>\n",
       "      <th>settlement_status</th>\n",
       "      <th>settlement_date</th>\n",
       "      <th>settlement_amount</th>\n",
       "      <th>settlement_percentage</th>\n",
       "      <th>settlement_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4975.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.65%</td>\n",
       "      <td>162.87</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.27%</td>\n",
       "      <td>59.83</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>Ryder</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>15.96%</td>\n",
       "      <td>84.33</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.49%</td>\n",
       "      <td>339.31</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>AIR RESOURCES BOARD</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>12.69%</td>\n",
       "      <td>67.79</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>University Medical Group</td>\n",
       "      <td>1 year</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt  funded_amnt  funded_amnt_inv        term int_rate  installment  \\\n",
       "0     5000.0       5000.0           4975.0   36 months   10.65%       162.87   \n",
       "1     2500.0       2500.0           2500.0   60 months   15.27%        59.83   \n",
       "2     2400.0       2400.0           2400.0   36 months   15.96%        84.33   \n",
       "3    10000.0      10000.0          10000.0   36 months   13.49%       339.31   \n",
       "4     3000.0       3000.0           3000.0   60 months   12.69%        67.79   \n",
       "\n",
       "  grade sub_grade                 emp_title emp_length       ...        \\\n",
       "0     B        B2                       NaN  10+ years       ...         \n",
       "1     C        C4                     Ryder   < 1 year       ...         \n",
       "2     C        C5                       NaN  10+ years       ...         \n",
       "3     C        C1       AIR RESOURCES BOARD  10+ years       ...         \n",
       "4     B        B5  University Medical Group     1 year       ...         \n",
       "\n",
       "  tax_liens  hardship_flag disbursement_method debt_settlement_flag  \\\n",
       "0       0.0              N                Cash                    N   \n",
       "1       0.0              N                Cash                    N   \n",
       "2       0.0              N                Cash                    N   \n",
       "3       0.0              N                Cash                    N   \n",
       "4       0.0              N                Cash                    N   \n",
       "\n",
       "  debt_settlement_flag_date settlement_status settlement_date  \\\n",
       "0                       NaN               NaN             NaN   \n",
       "1                       NaN               NaN             NaN   \n",
       "2                       NaN               NaN             NaN   \n",
       "3                       NaN               NaN             NaN   \n",
       "4                       NaN               NaN             NaN   \n",
       "\n",
       "  settlement_amount settlement_percentage settlement_term  \n",
       "0               NaN                   NaN             NaN  \n",
       "1               NaN                   NaN             NaN  \n",
       "2               NaN                   NaN             NaN  \n",
       "3               NaN                   NaN             NaN  \n",
       "4               NaN                   NaN             NaN  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read in csv files, store them\n",
    "dfs = [pd.read_csv(f,header=0,skiprows=1,low_memory=False) for f in filenames]\n",
    "\n",
    "# concatenate the dataframes (as standard there is only 1)\n",
    "df = pd.concat(dfs)\\\n",
    "        .dropna(subset=['loan_amnt'])\\\n",
    "        .dropna(axis=1, how='all')\n",
    "\n",
    "# View data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_rate_f</th>\n",
       "      <th>emp_length_f</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>charged_off</th>\n",
       "      <th>term_ 60 months</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "      <th>home_ownership_NONE</th>\n",
       "      <th>home_ownership_OTHER</th>\n",
       "      <th>home_ownership_OWN</th>\n",
       "      <th>home_ownership_RENT</th>\n",
       "      <th>verification_status_Source Verified</th>\n",
       "      <th>verification_status_Verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.65</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>27.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.96</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12252.0</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.49</td>\n",
       "      <td>10.0</td>\n",
       "      <td>49200.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>17.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_rate_f  emp_length_f  annual_inc    dti  charged_off  term_ 60 months  \\\n",
       "0       10.65          10.0     24000.0  27.65          0.0              0.0   \n",
       "1       15.27           0.0     30000.0   1.00          1.0              1.0   \n",
       "2       15.96          10.0     12252.0   8.72          0.0              0.0   \n",
       "3       13.49          10.0     49200.0  20.00          0.0              0.0   \n",
       "4       12.69           1.0     80000.0  17.94          0.0              1.0   \n",
       "\n",
       "   grade_B  grade_C  grade_D  grade_E  grade_F  grade_G  home_ownership_NONE  \\\n",
       "0      1.0      0.0      0.0      0.0      0.0      0.0                  0.0   \n",
       "1      0.0      1.0      0.0      0.0      0.0      0.0                  0.0   \n",
       "2      0.0      1.0      0.0      0.0      0.0      0.0                  0.0   \n",
       "3      0.0      1.0      0.0      0.0      0.0      0.0                  0.0   \n",
       "4      1.0      0.0      0.0      0.0      0.0      0.0                  0.0   \n",
       "\n",
       "   home_ownership_OTHER  home_ownership_OWN  home_ownership_RENT  \\\n",
       "0                   0.0                 0.0                  1.0   \n",
       "1                   0.0                 0.0                  1.0   \n",
       "2                   0.0                 0.0                  1.0   \n",
       "3                   0.0                 0.0                  1.0   \n",
       "4                   0.0                 0.0                  1.0   \n",
       "\n",
       "   verification_status_Source Verified  verification_status_Verified  \n",
       "0                                  0.0                           1.0  \n",
       "1                                  1.0                           0.0  \n",
       "2                                  0.0                           0.0  \n",
       "3                                  1.0                           0.0  \n",
       "4                                  1.0                           0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify loans of interest\n",
    "df = df.loc[df.loan_status.isin(['Fully Paid', 'Charged Off'])].copy()\n",
    "\n",
    "# Clean up variables \n",
    "df['charged_off'] = (df.loan_status=='Charged Off').astype(int)\n",
    "df['int_rate_f'] = df.int_rate.str[:-1].astype(float)\n",
    "df['emp_length_f'] = df.emp_length\\\n",
    "                        .str.split(' ')\\\n",
    "                        .str[0].str[:2]\\\n",
    "                        .str.replace('<','0')\\\n",
    "                        .astype(float)\n",
    "\n",
    "# label and features\n",
    "y_var = 'charged_off'\n",
    "X_vars = ['term', 'int_rate_f', 'grade', 'home_ownership', 'emp_length_f',\n",
    "          'annual_inc', 'verification_status', 'dti']\n",
    "\n",
    "# Create dummies\n",
    "data = pd.get_dummies(df[X_vars+[y_var]], drop_first=True)\\\n",
    "        .dropna()\\\n",
    "        .reset_index(drop=True)\\\n",
    "        .astype(np.float64)\\\n",
    "        .loc[:2000]\\\n",
    "        .copy()\n",
    "\n",
    "# View data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us split the data into test and training data. To do this we use the `StratifiedShuffleSplit` from scikit learn (read more about splitting methods [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=.3, random_state=3)\n",
    "\n",
    "# These are the row indices of the stratified split\n",
    "data_splits = list(sss.split(data[y_var], data[y_var]))\n",
    "\n",
    "# Separate data in y,X\n",
    "y = data[y_var]\n",
    "X_vars_b = data.columns!=y_var\n",
    "X = data.loc[:,X_vars_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1.1 Model validation\n",
    "\n",
    "We start out with some basic concepts and our goal is to estimate and evaluate the logistic regression.\n",
    "\n",
    "\n",
    "> **Ex. 1.1.1:** extract the first data split and construct features, target for train, test data.\n",
    ">> *Hint:* `data_splits` is a list with 10 train-test pairs of randomly selected row indices. So the k'th element of `data_splits` contains a pair (train_idx, test_idx) identifying the k'th fold in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.1 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = data_splits[0]\n",
    "\n",
    "y_train = y.loc[train_idx]\n",
    "X_train = X.loc[train_idx]\n",
    "\n",
    "y_test = y.loc[test_idx]\n",
    "X_test = X.loc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.2:** estimate a logistic regression model on your training data. Then compute the overall accuracy and $F_1$ score for the default class *on the training data*.\n",
    ">\n",
    ">> **Note**: the code below implements a pipeline which standardizes the data by converting it into zero mean and unit standard deviation. We let `C=10**10` which corresponds to no regularization. Make sure you understand why each of these steps are important by looking up the scikit learn docs.\n",
    ">\n",
    ">> *Hint 2:* `sklearn` has these functions built-in and are named `f1_score`, `accuracy_score`. See Raschka pp. 189-193."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler        # scales variables to be mean=0,sd=1\n",
    "from sklearn.linear_model import LogisticRegression     # regression model\n",
    "from sklearn.pipeline import Pipeline                   # For building our model pipeline\n",
    "\n",
    "lr = Pipeline([('scale', StandardScaler()),\n",
    "               ('clf', LogisticRegression(class_weight='balanced',C=10**10, solver = 'liblinear'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.2 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6628571428571428, 0.4025316455696203)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "y_p_tr = lr.predict(X_train)\n",
    "acc_tr, f1_tr = accuracy_score(y_p_tr,y_train), f1_score(y_p_tr,y_train)\n",
    "acc_tr, f1_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.3:** Explain some advantages of computing the $F_1$ score vs. computing the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.3 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** $F_1$ is a concrete measure based on actual predictions. The measure combines recall and precision by weighting them equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.4:** Compute the *test set* accuracy and $F_1$ score. How does these compare to the results you got in exercise 1.1.3? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.4 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.627287853577371, 0.33727810650887574)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p_ts = lr.predict(X_test)\n",
    "acc_tst, f1_tst = accuracy_score(y_p_ts,y_test), f1_score(y_p_ts,y_test)\n",
    "acc_tst, f1_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.5:** Explain why test set performance is often preferred to the training set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.5 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to avoid overfitting - the best way to do this is to hold out data to compare our estimates to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Cross validation\n",
    "\n",
    "We now turn to actually optimizing the model. We will use cross validation to perform the optimization.\n",
    "\n",
    "> **Ex. 1.1.6:** Explain what the parameter `C` does in logistic regression. How do parameters change when `C` get smaller? Also explain what `penalty='l1'` will do for coefficients.\n",
    ">> *Hint:* The documentation for scikit learn will tell you everything you need to know about their implementation of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.6 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "- `C` corresponds to $\\lambda^{-1}$ in the standard formulation of where $\\beta=\\arg\\min_{\\beta}MSE(\\beta)+ \\lambda\\sum |\\beta|^L$\n",
    "- in the above `l2` regularization is when $L=2$; this leads to smaller parameters\n",
    "- in the above `l1` regularization is when $L=1$; this leads to removal of parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.7:** Use cross validation on training set to estimate hyperparameters and then re-estimate model on training set. The set of $\\lambda$ to be considered are $\\{10^{-4},10^{-2},1,10^{2},10^{4}\\}$. Is the model with optimized hyperparameters better than the non-optimized?\n",
    ">\n",
    ">> *Hint 1:* This procedure is implemented in `GridSearchCV`. You set `n_jobs=-1` to parallize computation. See Raschka pp. 186-187 for inspiration.\n",
    ">\n",
    ">> *Hint 2:* Consider using `np.logspace` for making the set of $\\lambda$.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.7 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6339434276206323, 0.3373493975903615)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr_cv = GridSearchCV(estimator=lr,\n",
    "                     param_grid={'clf__C':np.logspace(-4,4,5)},\n",
    "                     n_jobs=-1,\n",
    "                     cv=3)\n",
    "lr_cv.fit(X_train, y_train)\n",
    "\n",
    "y_p_tst_cv = lr_cv.predict(X_test)\n",
    "acc_tst_cv = accuracy_score(y_p_tst_cv, y_test)\n",
    "f1_tst_cv = f1_score(y_p_tst_cv, y_test)\n",
    "acc_tst_cv, f1_tst_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.8:** Apply nested resampling to compute a distribution of test scores with and without optimization. You should use `data_splits` which we defined initially and input all the data.\n",
    ">\n",
    ">> *Hint:* You can implement this using your code from Ex. 1.1.6 and combine it with `cross_val_score`. Note that `cv` input should use `data_splits`. See Raschka pp. 188-189 for inspiration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.8 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This problem will be in assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.9:** Plot the distributions of the optimized vs. the non-optimized. What do you conclude about the performance difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.9 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e7c9369d30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEvJJREFUeJzt3X+QXeV93/H3x+KHseLAJGw7MWCkEHlc2cRKsgNO4zYpsbFsJojEOAG7MU4yJUylgdpxi9xSYkg8xXgMyUzUJjSDndpDNCRtExkU4yR1kkLrREssjGVblQAlqGTatU3ATPgh4W/+OEfMZbWrPftLK/G8XzM7Os9znuec52rP/ey5z73nnlQVkqQ2vGy5ByBJOnoMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDTljuAUx1+umn16pVq5Z7GJJ0XLn//vu/VlVjs7U75kJ/1apVTExMLPcwJOm4kuSvhrRzekeSGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUkGPu4izNT5I59/H+yFJ7Bp3pJ1mfZHeSvUk2H6HdpUkqyfhI3Qf7fruTvHUxBq3DVdW0P2dfe9eM6yS1Z9Yz/SQrgC3AW4D9wI4k26rqy1PavRK4Gvjzkbq1wGXA64BXAX+U5DVV9fziPQRJx7L5vAoFX4kulSFn+ucBe6vq4ap6DtgKbJim3S8BNwPPjNRtALZW1bNV9Qiwt9+epEbM51Wogb90hoT+GcCjI+X9fd0LknwfcFZV3TXXvpKko2dI6E/32uyFP8NJXgbcCvzCXPuObOPKJBNJJiYnJwcMSZI0H0NCfz9w1kj5TOCxkfIrgdcDf5JkH/BGYFv/Zu5sfQGoqtuqaryqxsfGZv06aEnSPA0J/R3AmiSrk5xE98bstkMrq+qJqjq9qlZV1Srg88DFVTXRt7ssyclJVgNrgL9Y9EchSRpk1k/vVNXBJJuAe4AVwO1VtSvJjcBEVW07Qt9dSe4EvgwcBDb6yR1JWj6DLs6qqu3A9il118/Q9kemlD8MfHie45MkLSK/hkGSGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktSQE5Z7AJJeGt5ww2d54ukDc+qzavPdc2p/6ikn8sAvXjinPnoxQ1/Sonji6QPsu+miJd3HXP9I6HBO70hSQwx9SWqIoS9JDTH0Jakhg0I/yfoku5PsTbJ5mvVXJXkwyc4k9yZZ29efmOS3+nVfSfLBxX4AkqThZg39JCuALcDbgLXA5YdCfcQdVXVuVa0DbgZu6evfCZxcVecCPwD8fJJVizR2SdIcDTnTPw/YW1UPV9VzwFZgw2iDqnpypLgSqEOrgJVJTgBOAZ4DRttKko6iIZ/TPwN4dKS8Hzh/aqMkG4H3AycBF/TVv0v3B+JvgFcA76uqbyxkwJKk+Rtypp9p6uqwiqotVXUOcC1wXV99HvA88CpgNfALSb77sB0kVyaZSDIxOTk5ePCSpLkZEvr7gbNGymcCjx2h/Vbgkn75XcBnqupAVf1/4D5gfGqHqrqtqsaranxsbGzYyCVJczYk9HcAa5KsTnIScBmwbbRBkjUjxYuAPf3yXwMXpLMSeCPw1YUPW5I0H7PO6VfVwSSbgHuAFcDtVbUryY3ARFVtAzYleTNwAHgcuKLvvgX4OPAlummij1fVF5fgcUiSBhj0hWtVtR3YPqXu+pHla2bo9xTdxzYlSccAr8iVpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGDLoiV8eGN9zwWZ54+sCc+63afPec2p96yok88IsXznk/ko59hv5x5ImnD7DvpouWfD9z/SMh6fjh9I4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUkEGhn2R9kt1J9ibZPM36q5I8mGRnknuTrB1Z971J/neSXX2bly/mA5AkDTfrjdGTrAC2AG8B9gM7kmyrqi+PNLujqn69b38xcAuwPskJwKeAn66qB5J8J3BgsR9EK175jzZz7m8d9jd3CfYDsPQ3YJd09M0a+sB5wN6qehggyVZgA/BC6FfVkyPtVwLVL18IfLGqHujbfX0xBt2qb37lJvbdtPRhvGrz3Uu+D0nLY8j0zhnAoyPl/X3diyTZmOQh4Gbg6r76NUAluSfJXyb5N9PtIMmVSSaSTExOTs7tEUiSBhsS+pmmrg6rqNpSVecA1wLX9dUnAG8C3t3/++NJfnSavrdV1XhVjY+NjQ0evCRpboaE/n7grJHymcBjR2i/FbhkpO+fVtXXqurvgO3A989noJKkhRsS+juANUlWJzkJuAzYNtogyZqR4kXAnn75HuB7k7yif1P3hxl5L0CSdHTN+kZuVR1MsokuwFcAt1fVriQ3AhNVtQ3YlOTNdJ/MeRy4ou/7eJJb6P5wFLC9qnyXUJKWyZBP71BV2+mmZkbrrh9ZvuYIfT9F97FNSdIy84pcSWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0Z9DUMkjSbo3FnN+/qtnCGvqRFcTTu7OZd3RbO6R1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQ7w46zhzNC5OOfWUE5d8H5KWh6F/HJnP1Y6rNt+95FdJSjp+OL0jSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDBoV+kvVJdifZm+Sw+6EluSrJg0l2Jrk3ydop61+d5KkkH1isgUuS5m7W0E+yAtgCvA1YC1w+NdSBO6rq3KpaB9wM3DJl/a3AHyzCeCVJCzDkTP88YG9VPVxVzwFbgQ2jDarqyZHiSqAOFZJcAjwM7Fr4cCVJCzEk9M8AHh0p7+/rXiTJxiQP0Z3pX93XrQSuBW5Y+FAlSQs1JPQzTV0dVlG1parOoQv56/rqG4Bbq+qpI+4guTLJRJKJycnJAUOSJM3HkC9c2w+cNVI+E3jsCO23Av+pXz4fuDTJzcBpwLeSPFNVvzbaoapuA24DGB8fP+wPiiRpcQwJ/R3AmiSrgf8LXAa8a7RBkjVVtacvXgTsAaiqfzLS5kPAU1MDX5J09Mwa+lV1MMkm4B5gBXB7Ve1KciMwUVXbgE1J3gwcAB4HrljKQUuS5mfQ9+lX1XZg+5S660eWrxmwjQ/NdXCSpMXlFbmS1BBDX5IaYuhLUkMMfUlqiKEvSQ0Z9OkdSRpi1ea7l3T7p55y4pJuvwWGvqRFse+mi+bUftXmu+fcRwvn9I4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDRkU+knWJ9mdZG+SzdOsvyrJg0l2Jrk3ydq+/i1J7u/X3Z/kgsV+AJKk4WYN/SQrgC3A24C1wOWHQn3EHVV1blWtA24Gbunrvwb8WFWdC1wBfHLRRi5JmrMhZ/rnAXur6uGqeg7YCmwYbVBVT44UVwLV13+hqh7r63cBL09y8sKHLUmajxMGtDkDeHSkvB84f2qjJBuB9wMnAdNN47wD+EJVPTuPcUqSFsGQ0M80dXVYRdUWYEuSdwHX0U3ndBtIXgd8BLhw2h0kVwJXArz61a8eMCRNlUz3a+rXfWT6+qrDfo2SXuKGTO/sB84aKZ8JPDZDW+imfy45VEhyJvDfgfdU1UPTdaiq26pqvKrGx8bGBgxJU1XVnH8ktWdI6O8A1iRZneQk4DJg22iDJGtGihcBe/r604C7gQ9W1X2LM2RJ0nzNGvpVdRDYBNwDfAW4s6p2JbkxycV9s01JdiXZSTevf2hqZxPwPcC/7z/OuTPJP1j8hyFJGmLInD5VtR3YPqXu+pHla2bo98vALy9kgJKkxeMVuZLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhg0I/yfoku5PsTbJ5mvVXJXkwyc4k9yZZO7Lug32/3UneupiDlyTNzayhn2QFsAV4G7AWuHw01Ht3VNW5VbUOuBm4pe+7FrgMeB2wHviP/fYkSctgyJn+ecDeqnq4qp4DtgIbRhtU1ZMjxZVA9csbgK1V9WxVPQLs7bcnSVoGJwxocwbw6Eh5P3D+1EZJNgLvB04CLhjp+/kpfc+Y10glHZeSzLzuIzP3q6qZV2rehpzpT/cbO+y3UVVbquoc4Frgurn0TXJlkokkE5OTkwOGJOl4UVXz+tHSGBL6+4GzRspnAo8dof1W4JK59K2q26pqvKrGx8bGBgxJkjQfQ0J/B7AmyeokJ9G9MbtttEGSNSPFi4A9/fI24LIkJydZDawB/mLhw5Ykzcesc/pVdTDJJuAeYAVwe1XtSnIjMFFV24BNSd4MHAAeB67o++5KcifwZeAgsLGqnl+ixyJJmkWOtbmz8fHxmpiYWO5hSNJxJcn9VTU+WzuvyJWkhhj6ktQQQ1+SGmLoS1JDjrk3cpNMAn+13ON4CTkd+NpyD0Kahsfm4jq7qma90OmYC30triQTQ97Rl442j83l4fSOJDXE0Jekhhj6L323LfcApBl4bC4D5/QlqSGe6UtSQwz9JZKkknxspPyBJB+apc8l09yKcr77X5fk7SPli6e7v/Es29ie5LQFjuNHkty1kG1oeSQ5M8nvJ9mT5KEkv9p/0+5M7U9L8i9Hyq9K8rtz3OeN/Zc3LkiSpxa6jZcqQ3/pPAv8RJLT59DnErr7EC+GdcALoV9V26rqprlsoKreXlV/u0jj0XEk3e2u/hvwe1W1BngN8G3Ah4/Q7TTghdCvqseq6tK57Leqrq+qP5rHkDXQkNslan4O0r1R9T7g342uSHI2cDswBkwCP0N3g5mLgR9Och3wjqp66Eh9quqvk3wCeIbu5vP/kO6WlZ8FbgROSfIm4D8ApwDjVbWp7/M08Frg7H7/VwA/CPx5Vb233+c+YBy4FLiqH8qpwL6q+mdJLgRuAE4GHurH9FSS9cCv0F1485cL+U/UsrkAeKaqPg5QVc8neR/wSJJHgLfS/d5XA3dU1Q3ATcA5SXYCfwhsAe6qqtcneS/dSc0K4PXAx+hurfrTdCdIb6+qb/TH5l3APuA3+7GsAF5fVUlyTr/dMeDvgH9RVV/t79dxB12mfWbp/lteAuZ7KzN/Zr3V21PAt9MdvKcCHwA+1K/7NHBFv/yzdGdTAJ8ALp1he0fq8xm6V21r6O5W9nLgvcCvjfR/odz32Up3O8sNwJPAuf027gfW9e32AaePbONE4H8CP0Z3NeWfASv7ddcC1/f7frQfS4A76Z74y/478WdOx+/VwK3T1H+hX/c3wHfSnUx8ie7kYBXwpZG2L5T7428v8Eq6wH4CuKpfdyvwr0aOzUun7POjwEf75T8G1vTL5wP/o1/eBrynX94IPLXc/4fH6o/TO0uoqp4E/gvdk2TUD9KdlQB8EnjTgM0dqc+dVfWtqtoDPEx3Bj+bT1f3DHkQ+H9V9WBVfQvYRfdknc6v0j3JPg28kW4q6r7+zO4KulcNrwUeqao9/fY/NWAsOvaEae5nPVL/h1X19ap6mm4aaMgx/Lmq+mZVTdKF/qf7+geZ4ZhL8pPA9wObk3wb8I+B3+mPud8Avqtv+kPAb/fLnxwwlmY5vbP0foVuiuPjR2gzn8/N1gzLQ7f3bP/vt0aWD5UPOy76l+dnA5sOVdE98S+f0m7dwP3r2LYLeMdoRZJvp7vn9fMs7JiDFx93Mx1zr6ObPvyn1U0vvQz426paN8P2Pe4G8Ex/iVXVN+imOH5upPp/0d1rGODdwL398jfpXv5OZ6Y+AO9M8rJ+vvO7gd2zbGtOkvwA3fTUP+9fDQB8HvihJN/Tt3lFktcAXwVW92MBuPywDep48MfAK5K8ByDJCrp5+E/QzaW/Jcl3JDmFbq7+Phb3mDuVbgryPf0rg0OvnB9J8s6+TZK8oe9yHy9+fmgGhv7R8TG6OfBDrgZ+JskX6d7Iuqav3wr86yRfGAnN2fpAF/J/CvwB3TzpM8DngLVJdib5qQWOfxPwHcDn+u39Zv9EfC/w2/2YPg+8tt/3lcDdSe7Fb0w9LvVTcz9Od0KxB/g/dB8Y+Ld9k3vpplF2Av+1qiaq6ut0031fSvLRBQ7hErpXlv+5P+Z29vXvBn4uyQN0r0Y29PXXABuT7KB7D00z8Irc49yhTztU1Zw+Dy3NVz/VN15Vm2Zrq2OPZ/qS1BDP9CWpIZ7pS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIb8PRcZJKTobObyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "cv_res = pd.DataFrame({'Not optimized':nested_cv_score_nonopt, \n",
    "                       'Optimized':nested_cv_score_opt})\n",
    "cv_res.plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Ex. 1.1.10** Argue how $F_1$ differs from  *area under the curve* (AUC). Comment on their theoretical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "- $F_1$ is a concrete measure based on actual predictions. \n",
    "- AUC is a theoretical measure which uses the predicted probabilities. For each possible threshold for predicting Positive we need to compute the  False Positive Rate (FPR) and True Positive Rate (TPR). AUC is the integral of the function that FPR maps into TPR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification trees and forests\n",
    "\n",
    ">  **Ex. 1.1.11** Estimate a classification tree on the training data (with default hyperparameters). Evalate both on training and test data by computing the *area under the curve*.\n",
    ">\n",
    ">> *Hint:* You can check out code for Ex. 1.1.10 for inspiration. You may also want to look up `roc_auc_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.11 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This problem will be in assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Ex. 1.1.12** Estimate 50 classification trees and compute the mean prediction. Use the mean prediction to compute the *area under the curve*.\n",
    ">\n",
    ">> *Hint:* You may use the code block below and apply it repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_tree(data_train):\n",
    "    '''\n",
    "    Trains a decision tree on a bootstrap \n",
    "    of training data.\n",
    "    '''\n",
    "    df_ = data_train.sample(frac=1, replace=True)\n",
    "    ct = DecisionTreeClassifier(class_weight='balanced')\n",
    "    ct.fit(df_.loc[:,X_vars_b], df_[y_var])\n",
    "    return ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.12 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bvq720\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9528985507246377\n",
      "0.5523338048090524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Train data\n",
    "data_train =  data.loc[train_idx,:]\n",
    "\n",
    "# Fit 50 decision trees\n",
    "CTs = [train_tree(data_train) for i in range(50)]\n",
    "\n",
    "# For both train and test do:\n",
    "for label, idxs in [('train', train_idx), ('test', test_idx)]:\n",
    "    \n",
    "    # 0s with shape n-samples x n-models\n",
    "    y_p_CT_ = np.zeros((idxs.shape[0], len(CTs)))\n",
    "    \n",
    "    # For each model index\n",
    "    for i in range(len(CTs)):\n",
    "        # Store models predicted values in the 0-matrix\n",
    "        y_p_CT_[:,i] = CTs[i].predict(X.loc[idxs])\n",
    "    # print average AUC\n",
    "    print(roc_auc_score(y_p_CT_.mean(1)>.2, y.loc[idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Ex. 1.1.13** Is Random Forest classification different from the procedure of aggregating tree predictions above? If so, explain how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.13 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Yes they are different. RF also bootstraps which features to use; this is called feature bagging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
