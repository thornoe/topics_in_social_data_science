{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 1:\n",
    "## Machine learning recap and sampling\n",
    "\n",
    "*Andreas Bjerre-Nielsen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "1. [This course](#This-course)\n",
    "2. [The why, what and how of machine learing](#Why-machine-learning) - see SDS L11\n",
    "3. Recap of ML\n",
    "    - [Regularization](#Regularization) - see SDS L12\n",
    "    - [Model validation](#Model-validation) - see SDS L12-13\n",
    "    - [Model building](#Model-building) - see SDS L13\n",
    "    - [Decision trees](#Decision-trees) - see SDS L14\n",
    "    - [Ensemble learning](#Ensemble-learning) - see SDS L14    \n",
    "4. [Advanced model validation](#Advanced-model-validation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course motivation\n",
    "\n",
    "The course has two teaching teaching agendas:\n",
    "- Learn tools for working with data\n",
    "    - Handle complex networks: friendships, banks, and much more..\n",
    "    - Process unstructured data, e.g. text, image, into model data\n",
    "    - Investigate spatial relations and objects \n",
    "- Learn modelling methods \n",
    "    - Advanced machine learning (inference, deep learning)\n",
    "    - Identification in networks and with spatial data\n",
    "    \n",
    "This course has synergies with other fields: \n",
    "- Economics: game theory, mechanism design etc.\n",
    "- Sociology: analysis of discourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course outline \n",
    "\n",
    "We teach in four blocks:\n",
    "- Block 1, machine learning: model validation, neural networks\n",
    "- Block 2, networks: structure, behavior and identification\n",
    "- Block 3, spatial data: theory and methods\n",
    "- Block 4, text data: natural language processing\n",
    "\n",
    "Each block builds on one or more of the previous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course projects (1)\n",
    "\n",
    "- Rules:\n",
    "    - At most four people\n",
    "    - Anything goes (next slide)\n",
    "    - More details on GitHub\n",
    "- Advice:\n",
    "    - Think about deep research questions. Which hypotheses?\n",
    "    - Consider method: prediction or identification?\n",
    "    - Talk to us in class or Kristian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course projects (2)\n",
    "\n",
    "- The world is your oyster:\n",
    "    - Last year scraping: central banks, conflict data, twitter/reddit, football statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What about all the data which is not open online?\n",
    "- Private data may be even more awesome.\n",
    "    - Get in touch with your employer\n",
    "    - Contact organizations, firms, goverment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value of modelling \n",
    "*Why are models useful?*\n",
    "\n",
    "Models are pursued with differens aims. Suppose we have a regression model, $y=X\\beta+\\epsilon$.\n",
    "\n",
    "- Social science:\n",
    "    - They teach us something about the world.\n",
    "    - We want unbiased estimate $\\hat{\\beta}$ and distribution\n",
    "- Data science:\n",
    "    - To make optimal future decisions and precise predictions, i.e. $\\hat{y}$.    \n",
    "    - Model flexibility\n",
    "        - Universal Approximation (e.g. for handwriting recognition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value of modelling (2)\n",
    "Which street is from a wealthy neighborhood?\n",
    "\n",
    "Street A | Street B\n",
    "- | - \n",
    "![alt](https://github.com/abjer/tsds/raw/master/material/1_intro_sampling/backyard-2116576_1280.jpg) | ![alt](https://github.com/abjer/tsds/raw/master/material/1_intro_sampling/luxury-home-2953371_1280.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value of modelling (3)\n",
    "\n",
    "Do you think machine can learn this difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Naik, N., Kominers, S. D., Raskar, R., Glaeser, E. L., & Hidalgo, C. A. (2017). Computer vision uncovers predictors of physical urban change. Proceedings of the National Academy of Sciences, 114(29), 7571-7576."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value of modelling (4)\n",
    "*Why the hype about machine learning in Social Science?*\n",
    "- Deep ideas: model validation, non-linear estimation\n",
    "- Used to construct input data\n",
    "    - E.g. parse text data, unstructured image data, network data\n",
    "- Combinination with causal methods:\n",
    "    - E.g. Athey, Wager (2018), Chernozhukov et al. 2017\n",
    "- Make predictions\n",
    "    - Useful in finance, macroeconomics\n",
    "    - Identifying susceptible candidates for policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learning\n",
    "*What do we mean by machine learning (ML)?*\n",
    "\n",
    "ML consists of two related phenomena\n",
    "\n",
    "- supervised learning\n",
    "    - assume target that is to be predicted/inferred\n",
    "    - scalar/number > regression \n",
    "    - categorical> classification \n",
    "    \n",
    "- unsupervised learning\n",
    "    - no target for classification\n",
    "    - includes clustering, component decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common supervised learning models\n",
    "Individual model\n",
    "- Linear/logistic regression  (SDS)\n",
    "    - no regularization (like econometrics)\n",
    "    - with regularization > next slides\n",
    "- Tree based methods (SDS/week 1)\n",
    "- Deep learning (week 2-4)\n",
    "\n",
    "Combining models\n",
    "- Ensemble, bagging  (SDS/week 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (1)\n",
    "\n",
    "*Why do we regularize?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To mitigate overfitting > better model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*How do we regularize?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We make models which are less complex:\n",
    "  - reducing the **number** of coefficient;\n",
    "  - reducing the **size** of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (2)\n",
    "\n",
    "*What does regularization look like?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We add a penalty term our optimization procedure:\n",
    "    \n",
    "$$ \\text{arg min}_\\beta \\, \\underset{\\text{MSE}}{\\underbrace{E[(y_0 - \\hat{f}(x_0))^2]}} + \\underset{\\text{penalty}}{\\underbrace{\\lambda \\cdot R(\\beta)}}$$\n",
    "\n",
    "Introduction of penalties implies that increased model complexity has to be met with high increases precision of estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (3)\n",
    "\n",
    "*What are some used penalty functions?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The two most common penalty functions are L1 and L2 regularization.\n",
    "\n",
    "- L1 regularization (***Lasso***): $R(\\beta)=\\sum_{j=1}^{p}|\\beta_j|$ \n",
    "    - Makes coefficients sparse, i.e. selects variables by removing some (if $\\lambda$ is high)\n",
    "    \n",
    "    \n",
    "- L2 regularization (***Ridge***): $R(\\beta)=\\sum_{j=1}^{p}\\beta_j^2$\n",
    "    - Reduce coefficient size\n",
    "    - Fast due to analytical solution\n",
    "    \n",
    "*To note:* The *Elastic Net* uses a combination of L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model performance\n",
    "*How do check our model fit?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One way is compute various measures of fit ($R^2$, accuracy etc.).\n",
    "    - Issue: adding more variable $\\Rightarrow$ higher $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*How is this solved?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use some of our sample for model evaluation. \n",
    "- Stagegy: divide into training data for estimation; remaining to test data for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measuring the problem\n",
    "*Does machine learning work out of the box?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In some cases ML works quite well out of the box.\n",
    "- Often ML requires making careful choices.\n",
    "    - Note that automated machine learning packages and services exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Which choices are to be made?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We need to pick model building **hyperparameters**.\n",
    "- E.g. $\\lambda$ for Lasso, Ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model validation (1)\n",
    "*How do we measure our model's performance for different hyperparameters?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember we cannot use the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Could we somehow mimick what we do with test data?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Yes, we can split the remaining non-test data into training and validation data:\n",
    "    - we train model for various hyperparameters on training data;\n",
    "    - pick the hyperparameters which performs best on validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model validation (2)\n",
    "*The non-test data is split into training and validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_02.png' alt=\"Drawing\" style=\"width: 800px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model pipelines (1)\n",
    "*Is there a smart way to build supervised ML models?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Build pipeline: \n",
    "- One step: preprocess data, estimate model\n",
    "- Ensures good practice - we only build model using training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model pipelines (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_01.png' alt=\"Drawing\" style=\"width: 900px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A hierarchal structure \n",
    "*What does a decision tree look like?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch03/images/03_17.png' alt=\"Drawing\" style=\"width: 700px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating decision trees\n",
    "*What can we conclude about the decision trees?*\n",
    "\n",
    "- Can fit anything ~ Universal Approximation\n",
    "    - *little* underfitting (~low bias)\n",
    "    - **LARGE** overfitting (~large variance)\n",
    "    \n",
    "- What can we say about linear and logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble of models\n",
    "Models can be combined into one; this is called **ensemble learning**.\n",
    "\n",
    "- Regression: use mean/median over model predictions \n",
    "- Classification: use mode of model predictions (i.e. most common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The wisdom of the crowd (1)\n",
    "\n",
    "What can we do to reduce overfitting of a decision tree?\n",
    "\n",
    "- We create multiple trees where for each tree\n",
    "    - We draw a subsample of observation\n",
    "    - We draw a subsample of features\n",
    "    \n",
    "These combined decision trees are called a random forest.\n",
    "- The predicted value is the mode (most common) predicted by the trees\n",
    "- Extension to regression where the mean over trees is computed.\n",
    "- Works almost like magic out of the box (has hyperparamters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The wisdom of the crowd (2)\n",
    "\n",
    "The underlying process of Random Forest is called **bagging**, short for *bootstrap aggregating*. Possible dimensions:\n",
    "- Bagging *observations*, i.e. 1st dimension of data\n",
    "- Bagging *features* (variables), i.e. 2nd dimension of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The holdout method\n",
    "*How do we got the more out of the data?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We reuse the train-test data split in reverse: \n",
    "- Rotate which parts of data is used for test and train.\n",
    "\n",
    "Advantage: We test on all the data; little extra computation.\n",
    "\n",
    "Disadvantage: Depends on the split; still only 50 pct. used for training model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Leave-one-out CV\n",
    "*How do we got the most of the data?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Procedure:\n",
    "- Each single observation as test data; remaining for training.\n",
    "- Also known as Jackknife\n",
    "\n",
    "Advantage: Robust, does not depend on random numbers!\n",
    "\n",
    "Disadvantages: \n",
    "- Very computing intensive: One model per observation. \n",
    "- Not good for hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K fold method (1)\n",
    "*How do balance computing time vs. overfitting?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We split the sample into $K$ even sized test bins.\n",
    "- For each test bin $k$ we use the remaining data for training.\n",
    "\n",
    "Advantages:\n",
    "- We use all our data for testing.\n",
    "- Training is done with 100-(100/K) pct. of the data, i.e. 90 pct. for K=10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K fold method (2)\n",
    "In K-fold cross validation we average the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_03.png' alt=\"Drawing\" style=\"width: 1100px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested cross validation\n",
    "What should we do if we have more than one model that we test? Is it okay to take the one that performs best on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- No, the performance of model may be biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Solution: \n",
    "- idea: perform cross-validation (CV) multiple times on different parts of data.\n",
    "\n",
    "- **outer CV**: \n",
    "    - split data like in cross validation\n",
    "    - for each training dataset perform **inner CV** to tune hyperparameters\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested cross validation (2)\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_07.png' alt=\"Drawing\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested cross validation (3)\n",
    "Improved measure of the uncertainty by re-doing cross-validation again and again. \n",
    "- called **Repeated k-fold Cross validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested resampling\n",
    "If we want to make reproducible research we should make repeated samples. Some possibilities:\n",
    "  \n",
    "- Subsampling:\n",
    "    - We randomly split data into train and test. Train data obs. are unique.    \n",
    "- The bootstrap:\n",
    "    - Draw training data with replacemtent from all data - same sample size.\n",
    "    - Unused data will be test data.\n",
    "    - Issue: Binder (2008) \"Adapting prediction error estimates for biased complexity selection in high-dimensional "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model comparison tests\n",
    "If we want to make reproducible research we should repeat do this many times.\n",
    "- Repeated subsampling: use corrected resampled t-test\n",
    "    - Nadeau, Claude, and Yoshua Bengio. \"Inference for the Generalization Error.\" Machine Learning 52.3 (2003): 239.\n",
    "- 5x2 nested cross validation: use resampled t-test\n",
    "    - Dietterich, Thomas G. \"Approximate statistical tests for comparing supervised classification learning algorithms.\" Neural computation 10.7 (1998): 1895-1923."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
